{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Exploring LangChain and LLM APIs for Building a RAG with Chroma Vector Store\n",
    "\n",
    "This notebook aims to explore the use of LangChain and LLM APIs in the context of building a Retrieval-Augmented Generation (RAG) system.  \n",
    "Specifically, we will utilize the Chroma vector store to process and retrieve relevant information from a dataset of 100 LLM papers, provided in PDF format.  \n",
    "The goal is to showcase how LangChain's tools, such as retrievers, document loaders, and LLM APIs, can be used to efficiently process large amounts of research papers and build a powerful information retrieval system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction]\n",
    "2. [Environment Setup]\n",
    "    - [Loading Environment Variables]\n",
    "3. [Libraries and Modules]\n",
    "4. [Document Processing]\n",
    "    - [Extracting Text from PDFs]\n",
    "    - [Preparing Documents from PDFs]\n",
    "5. [Vector Store Setup]\n",
    "    - [Creating or Loading Vector Store]\n",
    "6. [Retrieval and Query Processing]\n",
    "    - [Retrieving Relevant Documents]\n",
    "    - [Generating Answers using LLM]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports various modules and components from LangChain and other libraries to build a Retrieval-Augmented Generation (RAG) system:\n",
    "\n",
    "- **LangChain Hub**: For managing LangChain components and tools.\n",
    "- **RecursiveCharacterTextSplitter**: For splitting text into smaller chunks to facilitate processing.\n",
    "- **Chroma**: For storing and querying embeddings using a vector database.\n",
    "- **StrOutputParser**: For parsing string-based outputs from language models.\n",
    "- **RunnablePassthrough**: For creating pass-through components in a pipeline.\n",
    "- **ChatOpenAI**: For interacting with OpenAI's GPT models for chat-based generation.\n",
    "- **HuggingFaceEmbeddings**: For generating embeddings using Hugging Face models.\n",
    "- **OS**: For interacting with the operating system, including environment variables and paths.\n",
    "- **Document**: For representing and manipulating document objects in LangChain.\n",
    "- **pdfplumber**: For extracting text from PDF files.\n",
    "- **RetrievalQA**: For building a Retrieval-based Question Answering chain using retrievers and models.\n",
    "- **dotenv**: For loading environment variables from a `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For accessing LangChain's centralized hub for managing components and tools.\n",
    "from langchain import hub\n",
    "\n",
    "# For splitting text into smaller, manageable chunks for processing.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# For storing and querying embeddings using Chroma vector database.\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# For parsing string-based outputs from language model responses.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# For creating pass-through components in a pipeline (no modifications applied).\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# For interacting with OpenAI's GPT models for chat-based generation.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# For generating embeddings using Hugging Face models.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# For interacting with the operating system (e.g., environment variables, paths).\n",
    "import os\n",
    "\n",
    "# For representing and manipulating document objects in LangChain.\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# For extracting text content from PDF files.\n",
    "import pdfplumber\n",
    "\n",
    "# For creating a Retrieval-based Question Answering chain using retrievers and models.\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# For loading environment variables from a `.env` file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Execute environment variable loading immediately.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets the environment variables `LANGCHAIN_TRACING_V2`, `LANGCHAIN_ENDPOINT`, `LANGCHAIN_API_KEY`, and `OPENAI_API_KEY` by retrieving their values from the `.env` file or the system environment.\n",
    "\n",
    "- **LANGCHAIN_TRACING_V2**: For tracing and debugging LangChain operations.\n",
    "- **LANGCHAIN_ENDPOINT**: Specifies the endpoint for LangChain services.\n",
    "- **LANGCHAIN_API_KEY**: Provides the API key for LangChain usage.\n",
    "- **OPENAI_API_KEY**: Provides the API key for OpenAI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the environment variable 'LANGCHAIN_TRACING_V2','LANGCHAIN_ENDPOINT','LANGCHAIN_API_KEY','OPENAI_API_KEY' from the value stored in the .env file or system environment.\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines three functions:\n",
    "\n",
    "- **`extract_text_from_pdf(pdf_path)`**: Extracts text from a PDF file located at `pdf_path` using `pdfplumber`. It processes each page of the PDF and concatenates the extracted text into a single string.\n",
    "  \n",
    "- **`prepare_documents_from_pdfs(pdf_paths)`**: Processes a list of PDF file paths (`pdf_paths`) to extract their text content using `extract_text_from_pdf`. It creates a list of `Document` objects, each containing the extracted text and metadata about the source (PDF file path).\n",
    "\n",
    "- **`process_llm_response(llm_response)`**: Processes the response from a language model (LLM). It prints the main result from the LLM response and lists the sources of the information, which are the metadata of the documents used in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDFs\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Function to prepare documents from a list of PDF paths\n",
    "def prepare_documents_from_pdfs(pdf_paths):\n",
    "    documents = []\n",
    "    for pdf_path in pdf_paths:\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        if text:  # Only process if text is extracted\n",
    "            documents.append(Document(page_content=text, metadata={\"source\": pdf_path}))\n",
    "    return documents\n",
    "\n",
    "# Function which prints the main result from an LLM response and lists the sources of the information from the response's source documents.\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the following steps:\n",
    "\n",
    "- **Specify PDF directory**: Defines the directory containing PDF files (`./LLM_papers`) and generates a list of PDF file paths (`pdf_paths`).\n",
    "\n",
    "- **Set up persistence and embeddings**: Defines the path for the vector store's persistence directory (`./chroma_langchain_pdf_db`) and initializes Hugging Face embeddings using a pre-trained model (`sentence-transformers/all-mpnet-base-v2`).\n",
    "\n",
    "- **Load or create vector store**: Checks if a vector store already exists in the specified directory. If it exists, it loads the existing vector store. If not, it creates a new one by:\n",
    "  - Extracting text from the specified PDFs.\n",
    "  - Splitting the extracted text into manageable chunks using the `RecursiveCharacterTextSplitter`.\n",
    "  - Initializing and creating a new `Chroma` vector store using the processed documents and embeddings.\n",
    "\n",
    "- **Use the retriever**: After creating or loading the vector store, it sets up the `retriever` to query the vector store for relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store...\n",
      "Vector store returned Successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory containing your PDF files\n",
    "pdf_directory = \"./LLM_papers\"\n",
    "pdf_paths = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory) if f.endswith(\".pdf\")]\n",
    "\n",
    "# Path to persist directory\n",
    "persist_directory = \"./chroma_langchain_pdf_db\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Check if vectorstore exists, otherwise create it\n",
    "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vectorstore = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "else:\n",
    "    print(\"No existing vector store found. Creating a new one...\")\n",
    "\n",
    "    # Extract and prepare documents\n",
    "    print(\"Extracting text from PDFs...\")\n",
    "    documents = prepare_documents_from_pdfs(pdf_paths)\n",
    "\n",
    "    # Split documents into manageable chunks\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Initialize embeddings\n",
    "    print(\"Initializing embeddings...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory,\n",
    "    )\n",
    "    print(\"Vector store created and saved locally.\")\n",
    "\n",
    "print(\"Vector store returned Successfully.\")\n",
    "\n",
    "# Use the retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "This code demonstrates how to combine a retriever and a language model (LLM) to generate answers based on retrieved documents. Here's the breakdown:\n",
    "\n",
    "1. **Prompt**: \n",
    "   - The prompt template is pulled from the `rlm/rag-prompt` using LangChain's hub.\n",
    "\n",
    "2. **LLM**: \n",
    "   - A `ChatOpenAI` model (GPT-3.5-turbo) is used to generate answers with a temperature of 0 for deterministic results.\n",
    "\n",
    "3. **Post-processing**: \n",
    "   - The `format_docs` function is defined to format the documents retrieved by the retriever into a single string, joining their contents with newlines.\n",
    "\n",
    "4. **Chain**: \n",
    "   - A LangChain pipeline (`rag_chain`) is created using a combination of components:\n",
    "     - The retriever fetches relevant documents.\n",
    "     - The `format_docs` function formats these documents.\n",
    "     - The formatted documents and the question are fed into the prompt.\n",
    "     - The LLM generates an answer.\n",
    "     - The `StrOutputParser` parses the final output.\n",
    "\n",
    "5. **Question**: \n",
    "   - The `rag_chain` is invoked with a question: \"When training pre-trained Language Models, how is Matrix Decomposition utilized?\" The pipeline retrieves relevant documents, formats them, and generates the answer.\n",
    "\n",
    "This setup integrates retrieval-augmented generation (RAG) using a chain of operations to improve the accuracy and context of the language model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Matrix Decomposition is utilized in training pre-trained Language Models for model compression. Singular Value Decomposition (SVD) is a popular strategy for compressing large matrices into smaller ones, approximating learned matrices with fewer parameters. Fisher information is introduced to weigh the importance of parameters in matrix factorization to align the objective with task accuracy.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"When training pre trained Language Models, how is Matrix Decomposition utilized?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the following tasks:\n",
    "\n",
    "- **Retrieves relevant documents**: It uses a `RetrievalQA` chain that takes a query, retrieves relevant documents using the `retriever`, and generates an answer with the provided language model (`llm`).\n",
    "  \n",
    "- **Generates an answer**: The `qa_chain` is responsible for processing the query and generating an answer by combining the retrieved documents with the LLM's capabilities.\n",
    "\n",
    "- **Prints answer and sources**: The function `process_llm_response` is called to print the generated answer (`llm_response['result']`) and the sources used to generate the answer by listing the document metadata of the so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix decomposition is utilized in training pre-trained language models for model compression. Specifically, techniques like Singular Value Decomposition (SVD) are used to factorize large matrices into smaller matrices, reducing the number of parameters in the model. This compression strategy helps approximate the learned matrix with fewer parameters, ultimately reducing the size of the model while retaining important information.\n",
      "\n",
      "\n",
      "Sources:\n",
      "./LLM_papers\\2020.aacl-main.88.pdf\n",
      "./LLM_papers\\2207.00112.pdf\n",
      "./LLM_papers\\2211.09718.pdf\n",
      "./LLM_papers\\2109.06243.pdf\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "Takes a query , retrieves relevant documents using a retriever, \n",
    "generates an answer using an LLM, \n",
    "and prints both the answer and the sources used to generate it. \n",
    "'''\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "\n",
    "query = \"When training pre trained Language Models, how is Matrix Decomposition utilized?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
